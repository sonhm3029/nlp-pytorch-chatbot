{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tuannm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import underthesea\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return underthesea.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_worlds):\n",
    "    \"\"\"\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [0,     1,      0,      1,      0,      0,      0]\n",
    "    \"\"\"\n",
    "    bag = np.zeros(len(all_worlds), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_worlds):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "    return bag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 patterns\n",
      "2 tags: ['normal', 'toxic']\n",
      "133 unique stemmed words: ['+', ',', '1', '2', 'A', 'A tác dụng', 'Anh', 'B', 'Bạn', 'Bực hết', 'CHo', 'Chat', 'Cho', 'Chào', 'Chán', 'Chị', 'Con', 'Cách', 'Căng thẳng', 'Hello', 'Hi', 'Hôm nay', 'Hôm qua', 'Hợp chât', 'Ivivi', 'Làm', 'Màn hình', 'Mày', 'Mả', 'Nhà', 'Nhà nước', 'Nói', 'Phương pháp', 'Sao', 'Stupid', 'Thời buổi', 'Tiên sư', 'Trả lời', 'Tôi', 'Việt Nam', 'Xin', 'Xì', 'biết', 'bot', 'buồn', 'bây giờ', 'bạn', 'bắt', 'bằng', 'bị', 'bố mày', 'cha', 'chao', 'chatbot', 'chào', 'chó', 'con', 'cách', 'có', 'cướp', 'cả', 'cậu', 'cứt', 'diêm', 'fuck', 'giết', 'gì', 'hấp', 'hợp chât', 'khác', 'không', 'kém', 'luôn', 'là', 'lắm', 'lợn', 'mang', 'mà', 'mày', 'mình', 'mấy', 'mẹ', 'một', 'ngoan', 'ngu', 'người', 'như', 'nhỉ', 'này', 'nói', 'phần', 'quá', 'rất', 'sai', 'shit', 'thoát', 'thông minh', 'thông tin', 'thế', 'thế nào', 'tiếp đón', 'tiệm', 'to', 'toàn', 'trét', 'tuổi', 'tên', 'tôi', 'tội', 'vcl', 'vàng', 'vãi', 'vậy', 'về', 'xìn', 'you', 'yêu', 'Áp lực', 'ăn cỗ', 'Đau đầu', 'Đố', 'Đồ', 'đau đầu', 'đi', 'đái', 'đáng', 'đây', 'đói', 'đất', 'đầu', 'để', 'ơi', 'ấy']\n",
      "133 2\n",
      "Epoch [100/1000], Loss: 0.0054\n",
      "Epoch [200/1000], Loss: 0.0027\n",
      "Epoch [300/1000], Loss: 0.0000\n",
      "Epoch [400/1000], Loss: 0.0000\n",
      "Epoch [500/1000], Loss: 0.0001\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0000\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.0000\n",
      "final loss: 0.0000\n",
      "training complete. File saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "with open(r'/home/tuannm/sonhoang/nlp-pytorch-chatbot/src/intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # add to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # add to our words list\n",
    "        all_words.extend(w)\n",
    "        # add to xy pair\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# stem and lower each word\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [w for w in all_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(xy), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_words), \"unique stemmed words:\", all_words)\n",
    "\n",
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the model\n",
    "\n",
    "data = {\n",
    "    \"model_state\":model.state_dict(),\n",
    "    \"input_size\":input_size,\n",
    "    \"output_size\":output_size,\n",
    "    \"hidden_size\":hidden_size,\n",
    "    \"all_words\":all_words,\n",
    "    \"tags\":tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data,FILE)\n",
    "\n",
    "print(f\"training complete. File saved to {FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
